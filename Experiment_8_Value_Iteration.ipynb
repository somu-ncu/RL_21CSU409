{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy04y3XyV+0oSnwtYsk8SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somu-ncu/RL_21CSU409/blob/main/Experiment_8_Value_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzZ0RMBk9Fs_",
        "outputId": "bb50b99d-501a-4427-e97c-943fa2c64fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Value Matrix:\n",
            "[[0.    0.    1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.    0.81  0.729]]\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.    0.9   1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.729 0.81  0.729]]\n",
            "Iteration 2 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "Iteration 3 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "\n",
            "Optimal Value Function:\n",
            "State (0, 0): 0.810\n",
            "State (0, 1): 0.900\n",
            "State (0, 2): 1.000\n",
            "State (0, 3): 1.000\n",
            "State (1, 0): 0.729\n",
            "State (1, 2): 0.900\n",
            "State (1, 3): -1.000\n",
            "State (2, 0): 0.656\n",
            "State (2, 1): 0.729\n",
            "State (2, 2): 0.810\n",
            "State (2, 3): 0.729\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP (Transition probabilities, rewards, discount factor)\n",
        "num_states = 11\n",
        "num_actions = 4\n",
        "\n",
        "# Define the state space\n",
        "states = [(i, j) for i in range(3) for j in range(4) if not (i == 1 and j == 1)]\n",
        "\n",
        "# Define the rewards\n",
        "rewards = {\n",
        "    (0, 3): 1,\n",
        "    (1, 3): -1,\n",
        "}\n",
        "\n",
        "# Define the transition probabilities\n",
        "def transition_probabilities(state, action):\n",
        "    i, j = state\n",
        "    if state in rewards:\n",
        "        return [(state, 1.0)]  # Terminal state\n",
        "    if action == 'Up':\n",
        "        next_state = (max(i - 1, 0), j)\n",
        "    elif action == 'Down':\n",
        "        next_state = (min(i + 1, 2), j)\n",
        "    elif action == 'Left':\n",
        "        next_state = (i, max(j - 1, 0))\n",
        "    elif action == 'Right':\n",
        "        next_state = (i, min(j + 1, 3))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid action\")\n",
        "    if next_state not in states:\n",
        "        return [(state, 1.0)]  # Stay in the current state if the action leads to an invalid state\n",
        "    return [(next_state, 1.0)]\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "def value_iteration(states, rewards, gamma, theta):\n",
        "    V = {state: 0 for state in states}\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[state]\n",
        "            action_values = {}\n",
        "            if state in rewards:\n",
        "                continue\n",
        "            for action in ['Up', 'Down', 'Left', 'Right']:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                action_values[action] = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "            V[state] = max(action_values.values())\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "\n",
        "        # Print the current value matrix\n",
        "        print(f\"Iteration {iteration} - Value Matrix:\")\n",
        "        value_matrix = np.zeros((3, 4))\n",
        "        for state, value in V.items():\n",
        "            i, j = state\n",
        "            value_matrix[i][j] = value\n",
        "        print(value_matrix)\n",
        "        iteration += 1\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Set the value of the goal state to its reward after convergence\n",
        "    for goal_state, reward in rewards.items():\n",
        "        V[goal_state] = reward\n",
        "\n",
        "    return V\n",
        "\n",
        "theta = 0.0001\n",
        "\n",
        "# Run value iteration\n",
        "optimal_values = value_iteration(states, rewards, gamma, theta)\n",
        "\n",
        "# Print the final value function\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for state, value in optimal_values.items():\n",
        "    value = value\n",
        "    print(f\"State {state}: {value:.3f}\")"
      ]
    }
  ]
}